{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9750b92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7022d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb17ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda:0\")\n",
    "    print(\"Running GPU\")\n",
    "else :\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(\"Running CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19023186",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Hey , my name is Ayush.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6e2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= RobertaTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2202f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer(text, max_length=50, add_special_tokens=True, truncation=True, padding='max_length',\n",
    "                                     return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b5ec2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 13368,  2156,   127,   766,    16,  5847,  3810,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43f909d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_model= RobertaModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a10f07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.squeeze(text_tokens['input_ids'], 1)\n",
    "input_mask = torch.squeeze(text_tokens['attention_mask'], 1)\n",
    "\n",
    "text = text_model(input_ids=input_ids, attention_mask=input_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80de9d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1630, -0.1860,  0.0839,  ...,  0.2400,  0.0118,  0.1278],\n",
       "         [ 0.0159, -0.1894, -0.8131,  ...,  0.0126, -0.5428,  0.3579],\n",
       "         [ 0.0833, -0.1348, -0.1241,  ...,  0.0914,  0.1474,  0.1169],\n",
       "         ...,\n",
       "         [-0.0053, -0.1525, -0.5218,  ...,  0.0954, -0.4497,  0.1954],\n",
       "         [-0.0053, -0.1525, -0.5218,  ...,  0.0954, -0.4497,  0.1954],\n",
       "         [-0.0053, -0.1525, -0.5218,  ...,  0.0954, -0.4497,  0.1954]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41739e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634f2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
